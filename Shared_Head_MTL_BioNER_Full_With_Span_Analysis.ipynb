{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f62667",
   "metadata": {},
   "source": [
    "# ðŸ”— Shared-Head Multi-Task BioNER with Evaluation + Span Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers seqeval\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd54fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ner_tsv(path):\n",
    "    sentences, labels = [], []\n",
    "    tokens, tags = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                token, tag = line.split()\n",
    "                tokens.append(token)\n",
    "                tags.append(tag)\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(tags)\n",
    "    return {\"tokens\": sentences, \"tags\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fecf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/content\"  # adjust this path\n",
    "datasets = {}\n",
    "for name in [\"BC5CDR\", \"BC4CHEMD\", \"NCBI\", \"JNLPBA\"]:\n",
    "    datasets[name] = read_ner_tsv(f\"{base_path}/{name}-IOBES/train.tsv\")\n",
    "\n",
    "# Combine\n",
    "all_tokens, all_tags = [], []\n",
    "for d in datasets.values():\n",
    "    all_tokens.extend(d[\"tokens\"])\n",
    "    all_tags.extend(d[\"tags\"])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"tokens\": all_tokens, \"tags\": all_tags})\n",
    "})\n",
    "\n",
    "label_list = sorted(list({label for seq in all_tags for label in seq}))\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(example[\"tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b886bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./shared_head_mtl_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59038cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test sets\n",
    "def load_test_sets(base_path=\"/content\"):\n",
    "    return {name: read_ner_tsv(f\"{base_path}/{name}-IOBES/test.tsv\") for name in [\"BC5CDR\", \"BC4CHEMD\", \"NCBI\", \"JNLPBA\"]}\n",
    "\n",
    "def tokenize_test_set(test_data):\n",
    "    dataset = Dataset.from_dict(test_data)\n",
    "    return dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "test_sets = load_test_sets()\n",
    "tokenized_test_sets = {k: tokenize_test_set(v) for k, v in test_sets.items()}\n",
    "\n",
    "def evaluate_model_on_testset(task_name, raw_data, tokenized_data):\n",
    "    predictions = trainer.predict(tokenized_data)\n",
    "    pred_ids = np.argmax(predictions.predictions, axis=2)\n",
    "    labels = predictions.label_ids\n",
    "    tokens = raw_data[\"tokens\"]\n",
    "\n",
    "    pred_labels, true_labels = [], []\n",
    "    for i in range(len(labels)):\n",
    "        word_pointer = 0\n",
    "        p_seq, l_seq = [], []\n",
    "        for j, label_id in enumerate(labels[i]):\n",
    "            if label_id == -100: continue\n",
    "            if word_pointer < len(tokens[i]):\n",
    "                true_label = id_to_label[label_id]\n",
    "                pred_label = id_to_label[pred_ids[i][j]]\n",
    "                l_seq.append(true_label)\n",
    "                p_seq.append(pred_label)\n",
    "                word_pointer += 1\n",
    "        true_labels.append(l_seq)\n",
    "        pred_labels.append(p_seq)\n",
    "\n",
    "    y_true = [y for x in true_labels for y in x]\n",
    "    y_pred = [y for x in pred_labels for y in x]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"ðŸ“Š {task_name} â€” P: {precision:.2f} R: {recall:.2f} F1: {f1:.2f}\")\n",
    "    return precision, recall, f1\n",
    "\n",
    "results = {}\n",
    "for task in test_sets:\n",
    "    p, r, f1 = evaluate_model_on_testset(task, test_sets[task], tokenized_test_sets[task])\n",
    "    results[task] = {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "# Plot results\n",
    "tasks = list(results.keys())\n",
    "f1s = [results[t][\"f1\"] for t in tasks]\n",
    "prec = [results[t][\"precision\"] for t in tasks]\n",
    "rec = [results[t][\"recall\"] for t in tasks]\n",
    "\n",
    "x = range(len(tasks))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, prec, width=0.2, label='Precision', align='center')\n",
    "plt.bar([p + 0.2 for p in x], rec, width=0.2, label='Recall', align='center')\n",
    "plt.bar([p + 0.4 for p in x], f1s, width=0.2, label='F1 Score', align='center')\n",
    "plt.xticks([p + 0.2 for p in x], tasks)\n",
    "plt.xlabel(\"Task\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"ðŸ“ˆ Evaluation on Test Sets (Shared-Head MTL)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6296e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ Span-Length Sensitivity Analysis\n",
    "\n",
    "def extract_spans(tokens, labels):\n",
    "    spans = []\n",
    "    current = []\n",
    "    current_type = None\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current:\n",
    "                spans.append((\" \".join(current), len(current), current_type))\n",
    "            current = [tokens[i]]\n",
    "            current_type = label[2:]\n",
    "        elif label.startswith(\"I-\") or label.startswith(\"E-\"):\n",
    "            current.append(tokens[i])\n",
    "        elif label.startswith(\"S-\"):\n",
    "            spans.append((tokens[i], 1, label[2:]))\n",
    "            current = []\n",
    "        else:\n",
    "            if current:\n",
    "                spans.append((\" \".join(current), len(current), current_type))\n",
    "                current = []\n",
    "    if current:\n",
    "        spans.append((\" \".join(current), len(current), current_type))\n",
    "    return spans\n",
    "\n",
    "def span_length_evaluation(task_name, raw_data, tokenized_data):\n",
    "    predictions = trainer.predict(tokenized_data)\n",
    "    pred_ids = np.argmax(predictions.predictions, axis=2)\n",
    "    labels = predictions.label_ids\n",
    "    tokens_list = raw_data[\"tokens\"]\n",
    "\n",
    "    true_labels_list, pred_labels_list = [], []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        word_pointer = 0\n",
    "        p_seq, l_seq = [], []\n",
    "        for j, label_id in enumerate(labels[i]):\n",
    "            if label_id == -100: continue\n",
    "            if word_pointer < len(tokens_list[i]):\n",
    "                l_seq.append(id_to_label[label_id])\n",
    "                p_seq.append(id_to_label[pred_ids[i][j]])\n",
    "                word_pointer += 1\n",
    "        true_labels_list.append(l_seq)\n",
    "        pred_labels_list.append(p_seq)\n",
    "\n",
    "    length_bins = [1, 2, 3, '4+']\n",
    "    true_by_len = defaultdict(set)\n",
    "    pred_by_len = defaultdict(set)\n",
    "\n",
    "    for idx in range(len(tokens_list)):\n",
    "        true_spans = extract_spans(tokens_list[idx], true_labels_list[idx])\n",
    "        pred_spans = extract_spans(tokens_list[idx], pred_labels_list[idx])\n",
    "\n",
    "        for span in true_spans:\n",
    "            l = span[1]\n",
    "            bin_key = l if l <= 3 else '4+'\n",
    "            true_by_len[bin_key].add((idx, span[0], span[2]))\n",
    "\n",
    "        for span in pred_spans:\n",
    "            l = span[1]\n",
    "            bin_key = l if l <= 3 else '4+'\n",
    "            pred_by_len[bin_key].add((idx, span[0], span[2]))\n",
    "\n",
    "    # Metrics per span length\n",
    "    print(f\"\\nðŸ“Š Span Length Sensitivity for Task: {task_name}\")\n",
    "    print(\"Length\\tPrec\\tRec\\tF1\\tSupport\")\n",
    "    precs, recs, f1s = [], [], []\n",
    "    for bin_key in length_bins:\n",
    "        gold = true_by_len[bin_key]\n",
    "        pred = pred_by_len[bin_key]\n",
    "        tp = len(gold & pred)\n",
    "        fp = len(pred - gold)\n",
    "        fn = len(gold - pred)\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        support = len(gold)\n",
    "        precs.append(precision)\n",
    "        recs.append(recall)\n",
    "        f1s.append(f1)\n",
    "        print(f\"{bin_key}\\t{precision:.2f}\\t{recall:.2f}\\t{f1:.2f}\\t{support}\")\n",
    "\n",
    "    # Plot\n",
    "    x = range(len(length_bins))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x, precs, width=0.2, label='Precision', align='center')\n",
    "    plt.bar([p + 0.2 for p in x], recs, width=0.2, label='Recall', align='center')\n",
    "    plt.bar([p + 0.4 for p in x], f1s, width=0.2, label='F1 Score', align='center')\n",
    "    plt.xticks([p + 0.2 for p in x], [str(l) for l in length_bins])\n",
    "    plt.xlabel(\"Entity Span Length (tokens)\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(f\"Span Length Sensitivity â€” {task_name}\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ðŸ” Run span analysis per task\n",
    "for task in test_sets:\n",
    "    span_length_evaluation(task, test_sets[task], tokenized_test_sets[task])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
